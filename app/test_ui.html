<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Assistant</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        body {
            font-family: Arial, sans-serif;
            background: #1a1a1a;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            flex: 1;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            padding: 20px;
            gap: 20px;
        }
        button {
            padding: 30px 60px;
            font-size: 1.5em;
            border: 3px solid #fff;
            border-radius: 50px;
            cursor: pointer;
            font-weight: bold;
            background: transparent;
            color: #fff;
            transition: all 0.3s ease;
        }
        button:disabled {
            opacity: 0.3;
            cursor: not-allowed;
        }
        button:hover:not(:disabled) {
            background: #fff;
            color: #1a1a1a;
            transform: scale(1.05);
        }
        #connectBtn.connected {
            background: #4caf50;
            color: white;
            border-color: #4caf50;
        }
        #connectBtn.connected:hover {
            background: #45a049;
            border-color: #45a049;
            color: white;
        }
        .transcript {
            width: 100%;
            max-width: 800px;
            flex: 1;
            padding: 20px;
            background: rgba(255, 255, 255, 0.05);
            border: 1px solid rgba(255, 255, 255, 0.1);
            border-radius: 20px;
            overflow-y: auto;
            font-size: 1em;
        }
        .transcript-item {
            margin: 12px 0;
            padding: 12px 20px;
            border-radius: 15px;
            line-height: 1.5;
        }
        .transcript-item.user {
            background: rgba(33, 150, 243, 0.2);
            text-align: right;
            color: #64b5f6;
            border: 1px solid rgba(33, 150, 243, 0.3);
        }
        .transcript-item.ai {
            background: rgba(76, 175, 80, 0.2);
            text-align: left;
            color: #81c784;
            border: 1px solid rgba(76, 175, 80, 0.3);
        }
        .transcript-label {
            font-weight: bold;
            font-size: 0.9em;
            margin-bottom: 5px;
            opacity: 0.8;
        }

        /* Scrollbar styling */
        .transcript::-webkit-scrollbar {
            width: 8px;
        }
        .transcript::-webkit-scrollbar-track {
            background: rgba(255, 255, 255, 0.05);
            border-radius: 10px;
        }
        .transcript::-webkit-scrollbar-thumb {
            background: rgba(255, 255, 255, 0.2);
            border-radius: 10px;
        }
        .transcript::-webkit-scrollbar-thumb:hover {
            background: rgba(255, 255, 255, 0.3);
        }
    </style>
</head>
<body>
    <div class="container">
        <button id="connectBtn">START</button>
        <div class="transcript" id="transcript"></div>
    </div>

    <script>
        let ws = null;
        let audioContext = null;
        let isRecording = false;
        let mediaStream = null;
        let processor = null;

        // Audio playback queue to prevent overlapping
        let audioQueue = [];
        let isPlayingAudio = false;
        let currentAudioSource = null;

        // Reconnection logic
        let reconnectAttempts = 0;
        const MAX_RECONNECT_ATTEMPTS = 5;
        const RECONNECT_DELAY = 2000;
        let reconnectTimeout = null;
        let shouldReconnect = false;

        const connectBtn = document.getElementById('connectBtn');
        const transcriptDiv = document.getElementById('transcript');

        // Track conversation order
        let pendingUserTranscript = null;

        // Add transcript to UI
        function addTranscript(text, type) {
            const item = document.createElement('div');
            item.className = `transcript-item ${type}`;

            const label = document.createElement('div');
            label.className = 'transcript-label';
            label.textContent = type === 'user' ? 'You:' : 'AI:';

            const content = document.createElement('div');
            content.textContent = text;

            item.appendChild(label);
            item.appendChild(content);
            transcriptDiv.appendChild(item);

            // Auto-scroll to bottom
            transcriptDiv.scrollTop = transcriptDiv.scrollHeight;
        }

        // Add user transcript with proper timing
        function addUserTranscript(text) {
            // If there's a pending placeholder, update it in place
            if (pendingUserTranscript) {
                const content = pendingUserTranscript.querySelector('div:last-child');
                if (content) {
                    content.textContent = text;
                    pendingUserTranscript.style.opacity = '1';
                }
                pendingUserTranscript = null;
            } else {
                // No placeholder, add normally (shouldn't happen in normal flow)
                addTranscript(text, 'user');
            }
        }

        // Add placeholder when user stops speaking
        function addUserPlaceholder() {
            const item = document.createElement('div');
            item.className = 'transcript-item user';
            item.style.opacity = '0.5';

            const label = document.createElement('div');
            label.className = 'transcript-label';
            label.textContent = 'You:';

            const content = document.createElement('div');
            content.textContent = 'Transcribing...';

            item.appendChild(label);
            item.appendChild(content);
            transcriptDiv.appendChild(item);

            pendingUserTranscript = item;
            transcriptDiv.scrollTop = transcriptDiv.scrollHeight;
        }

        // Simple console logging for debugging
        function log(message, type = 'info') {
            const timestamp = new Date().toLocaleTimeString();
            const emoji = type === 'success' ? 'âœ…' : type === 'error' ? 'âŒ' : 'â„¹ï¸';
            console.log(`${emoji} [${timestamp}] ${message}`);
        }

        connectBtn.addEventListener('click', async () => {
            if (ws && ws.readyState === WebSocket.OPEN) {
                // Disconnect and cleanup
                shouldReconnect = false;
                if (reconnectTimeout) {
                    clearTimeout(reconnectTimeout);
                    reconnectTimeout = null;
                }

                // Full cleanup
                cleanupResources();

                // Clear transcript (like clearing chat history)
                transcriptDiv.innerHTML = '';

                // Close WebSocket
                ws.close();
                ws = null;

                // Reset state
                reconnectAttempts = 0;

                connectBtn.textContent = 'START';
                connectBtn.classList.remove('connected');
                log('Session ended. Tap START to begin new session.', 'info');
                return;
            }

            // Connect and auto-start
            shouldReconnect = true;
            connectToServer();
        });

        async function connectToServer() {
            try {
                // Disable button during connection
                connectBtn.disabled = true;
                log('Connecting to WebSocket server...');
                ws = new WebSocket('ws://localhost:8000/');

                ws.onopen = async () => {
                    log('Connected! Starting voice assistant...', 'success');
                    reconnectAttempts = 0; // Reset on successful connection
                    connectBtn.textContent = 'STOP';
                    connectBtn.classList.add('connected');
                    connectBtn.disabled = false;

                    // Auto-start recording after connection
                    await startRecording();
                };

                ws.onmessage = async (event) => {
                    try {
                        const data = JSON.parse(event.data);

                        if (data.type === 'audio_delta') {
                            // Queue audio for smooth playback
                            queueAudio(data.audio);
                        } else if (data.type === 'transcript_delta') {
                            // Log transcript for debugging
                            console.log('AI speaking:', data.text);
                        } else if (data.type === 'transcript_done') {
                            log(`AI: ${data.text}`, 'success');
                            addTranscript(data.text, 'ai');
                        } else if (data.type === 'user_transcript') {
                            log(`You: ${data.text}`, 'info');
                            addUserTranscript(data.text);
                        } else if (data.type === 'speech_started') {
                            log('Listening...', 'info');
                        } else if (data.type === 'speech_stopped') {
                            addUserPlaceholder();
                        } else if (data.type === 'session_created' || data.type === 'session_updated') {
                            log(`${data.type}`, 'success');
                        } else if (data.type === 'error') {
                            log(`Error: ${JSON.stringify(data.error)}`, 'error');
                        }
                    } catch (error) {
                        log(`Error parsing message: ${error.message}`, 'error');
                    }
                };

                ws.onerror = (error) => {
                    log('WebSocket error!', 'error');
                    console.error(error);
                };

                ws.onclose = (event) => {
                    log('Voice assistant stopped');
                    connectBtn.textContent = 'START';
                    connectBtn.classList.remove('connected');
                    connectBtn.disabled = false;

                    // Clear pending transcript placeholder
                    if (pendingUserTranscript) {
                        pendingUserTranscript.remove();
                        pendingUserTranscript = null;
                    }

                    if (isRecording) {
                        stopRecording();
                    }

                    // Auto-reconnect if not manually disconnected
                    if (shouldReconnect && reconnectAttempts < MAX_RECONNECT_ATTEMPTS) {
                        reconnectAttempts++;
                        log(`Reconnecting in ${RECONNECT_DELAY/1000}s... (${reconnectAttempts}/${MAX_RECONNECT_ATTEMPTS})`, 'info');
                        reconnectTimeout = setTimeout(connectToServer, RECONNECT_DELAY);
                    } else if (reconnectAttempts >= MAX_RECONNECT_ATTEMPTS) {
                        log('Connection failed. Please tap START to retry.', 'error');
                        shouldReconnect = false;
                    }
                };
            } catch (error) {
                log(`Connection error: ${error.message}`, 'error');
                connectBtn.disabled = false;

                // Retry on error
                if (shouldReconnect && reconnectAttempts < MAX_RECONNECT_ATTEMPTS) {
                    reconnectAttempts++;
                    reconnectTimeout = setTimeout(connectToServer, RECONNECT_DELAY);
                }
            }
        }

        async function startRecording() {
            try {
                log('ðŸŽ¤ Activating microphone...');
                mediaStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        sampleRate: 24000,
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true
                    }
                });

                if (!audioContext || audioContext.state === 'closed') {
                    audioContext = new AudioContext({ sampleRate: 24000 });
                }

                const source = audioContext.createMediaStreamSource(mediaStream);
                processor = audioContext.createScriptProcessor(4096, 1, 1);

                source.connect(processor);
                processor.connect(audioContext.destination);

                processor.onaudioprocess = (e) => {
                    if (!isRecording) return;

                    const inputData = e.inputBuffer.getChannelData(0);
                    const pcm16 = convertToPCM16(inputData);
                    const base64 = arrayBufferToBase64(pcm16);

                    if (ws && ws.readyState === WebSocket.OPEN) {
                        ws.send(JSON.stringify({
                            type: 'audio',
                            audio: base64
                        }));
                    }
                };

                isRecording = true;
                log('Voice assistant ready! Just start talking.', 'success');

            } catch (error) {
                log(`Microphone error: ${error.message}`, 'error');
            }
        }

        function stopRecording() {
            if (!isRecording) return;

            isRecording = false;
            log('Stopping voice assistant...');

            // Send stop signal to backend
            if (ws && ws.readyState === WebSocket.OPEN) {
                try {
                    ws.send(JSON.stringify({ type: 'stop' }));
                } catch (e) {
                    // WebSocket may be closed
                }
            }

            // Clear any pending user transcript placeholder
            if (pendingUserTranscript) {
                pendingUserTranscript.remove();
                pendingUserTranscript = null;
            }

            // Stop all audio playback
            stopAudioPlayback();

            // Stop media stream
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => {
                    try {
                        track.stop();
                    } catch (e) {
                        // Already stopped
                    }
                });
                mediaStream = null;
            }

            // Disconnect processor
            if (processor) {
                try {
                    processor.onaudioprocess = null;
                    processor.disconnect();
                } catch (e) {
                    // Already disconnected
                }
                processor = null;
            }
        }

        function convertToPCM16(float32Array) {
            const buffer = new ArrayBuffer(float32Array.length * 2);
            const view = new DataView(buffer);
            for (let i = 0; i < float32Array.length; i++) {
                const s = Math.max(-1, Math.min(1, float32Array[i]));
                view.setInt16(i * 2, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
            }
            return buffer;
        }

        function arrayBufferToBase64(buffer) {
            const bytes = new Uint8Array(buffer);
            let binary = '';
            for (let i = 0; i < bytes.byteLength; i++) {
                binary += String.fromCharCode(bytes[i]);
            }
            return btoa(binary);
        }

        // Queue audio chunk for playback
        function queueAudio(base64Audio) {
            audioQueue.push(base64Audio);
            if (!isPlayingAudio) {
                processAudioQueue();
            }
        }

        // Process audio queue sequentially
        async function processAudioQueue() {
            if (audioQueue.length === 0) {
                isPlayingAudio = false;
                return;
            }

            isPlayingAudio = true;
            const base64Audio = audioQueue.shift();

            try {
                await playAudioChunk(base64Audio);
            } catch (error) {
                log(`Audio playback error: ${error.message}`, 'error');
            }

            // Process next chunk
            processAudioQueue();
        }

        // Play single audio chunk
        async function playAudioChunk(base64Audio) {
            return new Promise((resolve, reject) => {
                try {
                    // Decode base64 to PCM16 data
                    const audioData = atob(base64Audio);
                    const pcm16Data = new Int16Array(audioData.length / 2);
                    const dataView = new DataView(new ArrayBuffer(audioData.length));

                    for (let i = 0; i < audioData.length; i++) {
                        dataView.setUint8(i, audioData.charCodeAt(i));
                    }

                    for (let i = 0; i < pcm16Data.length; i++) {
                        pcm16Data[i] = dataView.getInt16(i * 2, true);
                    }

                    // Convert PCM16 to Float32 for Web Audio API
                    const float32Data = new Float32Array(pcm16Data.length);
                    for (let i = 0; i < pcm16Data.length; i++) {
                        float32Data[i] = pcm16Data[i] / (pcm16Data[i] < 0 ? 0x8000 : 0x7FFF);
                    }

                    if (!audioContext || audioContext.state === 'closed') {
                        audioContext = new AudioContext({ sampleRate: 24000 });
                    }

                    // Create audio buffer and play
                    const audioBuffer = audioContext.createBuffer(1, float32Data.length, 24000);
                    audioBuffer.getChannelData(0).set(float32Data);

                    currentAudioSource = audioContext.createBufferSource();
                    currentAudioSource.buffer = audioBuffer;
                    currentAudioSource.connect(audioContext.destination);

                    currentAudioSource.onended = () => {
                        currentAudioSource = null;
                        resolve();
                    };

                    currentAudioSource.start(0);
                } catch (error) {
                    reject(error);
                }
            });
        }

        // Stop all audio playback
        function stopAudioPlayback() {
            audioQueue = [];
            isPlayingAudio = false;
            if (currentAudioSource) {
                try {
                    currentAudioSource.stop();
                    currentAudioSource.disconnect();
                } catch (e) {
                    // Already stopped
                }
                currentAudioSource = null;
            }
        }

        // Cleanup all resources
        function cleanupResources() {
            log('Cleaning up resources...');

            // Set recording flag to false first
            isRecording = false;

            // Stop audio playback
            stopAudioPlayback();

            // Clear pending user transcript
            if (pendingUserTranscript) {
                pendingUserTranscript.remove();
                pendingUserTranscript = null;
            }

            // Disconnect and cleanup processor first
            if (processor) {
                try {
                    processor.onaudioprocess = null;
                    processor.disconnect();
                } catch (e) {
                    // Already disconnected
                }
                processor = null;
            }

            // Stop media stream
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => {
                    try {
                        track.stop();
                        track.enabled = false;
                    } catch (e) {
                        // Already stopped
                    }
                });
                mediaStream = null;
            }

            // Close AudioContext completely for full reset
            if (audioContext && audioContext.state !== 'closed') {
                try {
                    audioContext.close();
                } catch (e) {
                    console.error('Error closing AudioContext:', e);
                }
            }
            audioContext = null;

            log('Cleanup complete', 'success');
        }

        // Cleanup on page unload
        window.addEventListener('beforeunload', () => {
            shouldReconnect = false;
            if (ws && ws.readyState === WebSocket.OPEN) {
                ws.close();
            }
            cleanupResources();
        });
    </script>
</body>
</html>
